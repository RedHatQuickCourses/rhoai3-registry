= Architecture: The AI Supply Chain
:navtitle: Architecture Deep Dive
:toc: macro

// Antora metadata
:page-role: architecture-concept
:description: Technical deep dive into the RHOAI Model Registry components, data flow, and separation of concerns.

[.lead]
*A "Registry" is not a "Repository." Understanding the difference is key to a scalable AI Platform.*

To build a private AI Factory, you must distinguish between **where the data lives** and **how the data is governed**. In Red Hat OpenShift AI (RHOAI), we decouple these two concerns to ensure performance, security, and flexibility.

== The Core Concept: Decoupling Control vs. Data

The RHOAI Model Registry architecture follows a "split-plane" design.

 * **The Control Plane (The "Librarian"):** The *Model Registry* itself. It is a lightweight metadata service backed by a relational database (MySQL). It knows *about* the models (Author, Version, License, Metrics) but does not hold the files.
 * **The Data Plane (The "Bookshelf"):** Your object storage (S3/MinIO/Quay/Ceph). This is where the heavy artifacts (Safetensors, ONNX files, configs) actually reside.


== Component Breakdown

=== 1. The Registry Service (Control Plane)
This is the brain of the operation. It exposes a REST API that allows users and systems to register, query, and update model metadata.


 * **Technology:** A Golang-based service managed by the `model-registry-operator`.
 * **Backend:** Requires a MySQL 8.x or MariaDB database.
 * **Function:** Enforces the schema. It ensures that every registered model has the required fields (e.g., you cannot register a model without a version number).

=== 2. The Artifact Storage (Data Plane)
This is the vault. It must be an S3-compatible object storage service.


 * **Technology:** AWS S3, Red Hat Ceph Storage, or MinIO (for this lab).
 * **Security:** This bucket should be **private**. Only the specific Service Accounts used by your pipelines and serving runtimes should have read access.
 * **The "Golden Image" Concept:** Once a model version is uploaded here and registered, the file should be treated as **immutable**.

=== 3. The Model Catalog (The "Showroom")
The Catalog is where AI Developers and Data Scientists browse available models. By default, it shows Red Hat Validated (ready for inference) and Red Hat AI models (ready for tuning pipeline). You'll find model like (Granite, DeepSeekR1 and Llama). To show *your* private models, we must explicitly connect the two.

 * **The Mechanism:** The RHOAI Dashboard reads from **Catalog Sources**.
 * **The Integration:** The Model Catalog does not use a database for configuration; it uses a Kubernetes **ConfigMap**. You will edit this object directly to add your model details.

 * **The Workflow:**
You need to define two logical "files" inside this ConfigMap:
 
 1.  **`sources.yaml`**: Registers your custom catalog path.
 2.  **`my-models.yaml`**: Contains the actual metadata for your models.

[%collapsible]
====
[source,bash]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: model-catalog-sources
  namespace: rhoai-model-registries
data:
  # FILE 1: The Main Config - Points to your custom file
  sources.yaml: |
    catalogs:
      - name: "Model-Registry-Models"
        id: "registry-custom-models"
        type: "yaml"
        properties:
          yamlCatalogPath: "/data/user-sources/registry-models.yaml"

  # FILE 2: Your Model Definitions (Add models here)
  registry-models.yaml: |
    models:
      # --- MODEL 1: IBM Granite ---
      - name: "granite-3.0-2b-instruct"
        description: "IBM Granite Small Language Model"
        customProperties:
          model_type:
            metadataType: MetadataStringValue
            string_value: "generative"
          author:
            metadataType: MetadataStringValue
            string_value: "IBM"
          version:
            metadataType: MetadataStringValue
            string_value: "1.0"
        artifacts:
          - name: "model-artifact"
            uri: "oci://quay.io/redhat-ai-services/modelcar-catalog:granite-3.0-2b-instruct"
----
====

== The Data Flow: Lifecycle of a Model

Understanding the flow for the hands-on lab you are about to perform.

1.  **Ingestion (The Download):**
    * A Python script authenticates with Hugging Face (or another source).
    * It downloads the raw files to a temporary workspace.

2.  **Storage (The Storage):**
    * The script uploads the files to your private S3 bucket (e.g., `s3://my-private-models/granite-7b/v1/`).
    * *Result:* The data is now safe and sovereign.

3.  **Registration (The Model Registry):**
    * The script calls the Model Registry API.
    * It sends a JSON payload: "I have a model named 'Granite', Version '1.0', located at 's3://...'"
    * *Result:* The Registry generates a unique ID and stamps the entry.

4.  **Visibility (The Model Catalog):**
    * A Data Scientist opens the RHOAI Dashboard.
    * They see the "Granite 7B" card.
    * When they click "Deploy," RHOAI reads the S3 URI from the Registry and pulls the weights directly from your private bucket—**never** touching the public internet.

5.  **Discovery (The Consumption):**
    * A Data Scientist opens the RHOAI Dashboard.
    * They see the "Granite 2B" card.
    * When they click "Deploy," RHOAI reads the S3 URI from the Registry and pulls the weights directly from your private bucket—**never** touching the public internet.

== System Requirements for the Lab

To build this architecture in the next module, your cluster must meet these minimums:

|===
| Component | Requirement | Role |
| **OpenShift AI** | v3.0 | The Platform |
| **Database** | MySQL v8.0 | Registry Backend |
| **Storage** | MinIO (or S3) | Artifact Storage |
| **Compute** | 2 vCPUs, 4GB RAM | For Registry & DB Pods |
|===
---
*Now that you understand the blueprint, it is time to pick up the tools.*

xref:section2.adoc[Next: The QuickStart Lab (Hands-On) >]
