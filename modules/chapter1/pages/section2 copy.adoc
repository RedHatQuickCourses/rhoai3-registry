= Lab: Automating Model Ingestion with Kubernetes Jobs
:toc: macro
:source-highlighter: rouge

In this lab, you will deploy a "Supply Chain" pipeline that runs entirely inside the cluster. Instead of running Python scripts on your laptop (which requires complex networking and dependencies), we will package the logic into a Kubernetes Job.

== Prerequisites

* **User Role:** General User (Project Editor) or Cluster Admin.
* **Namespace:** `rhoai-model-registry-lab`
* **Pre-conditions:**
** MinIO and MySQL must be running.
** The `aws-connection-minio` Secret must exist (created in the previous infrastructure step).
** The Registry DB Secret must exist (created manually in the previous step).

== Procedure

=== 1. Create the Ingestion Script

We will create a shell script `run_ingestion.sh` that generates the necessary Kubernetes objects:
1.  **ServiceAccount:** An identity for our Job to run as.
2.  **ConfigMap:** Stores our Python ingestion logic.
3.  **Job:** The execution engine that runs the code.

Create a file named `run_ingestion.sh` with the following content:

.run_ingestion.sh
[source,bash]
----
#!/bin/bash
set -e

# --- CONFIGURATION ---
NAMESPACE="rhoai-model-registry-lab"
MODEL_ID="Qwen/Qwen3-0.6B"
# Use internal cluster DNS for speed and security
REGISTRY_HOST="model-registry-lab.rhoai-model-registries.svc.cluster.local"
MINIO_HOST="minio-service.${NAMESPACE}.svc.cluster.local"
SERVICE_ACCOUNT="model-ingestion-sa"

echo "ðŸš€ Preparing Supply Chain Job for $MODEL_ID..."

# -----------------------------------------------------------------------------
# 1. Create Service Account
# We use a dedicated identity for this job for better audit trails.
# -----------------------------------------------------------------------------
echo "âž¤ Ensuring ServiceAccount '$SERVICE_ACCOUNT' exists..."
cat <<EOF | oc apply -n $NAMESPACE -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: $SERVICE_ACCOUNT
  labels:
    app: model-supply-chain
EOF

# -----------------------------------------------------------------------------
# 2. Embed the Python Logic (ConfigMap)
# -----------------------------------------------------------------------------
cat <<EOF > ingest_and_register.py
import os
import boto3
from huggingface_hub import snapshot_download
from model_registry import ModelRegistry
from botocore.client import Config

# --- CONFIGURATION ---
MODEL_ID = "${MODEL_ID}"
VERSION = "1.0.0"
S3_BUCKET = "private-models"

# Env vars provided by the Job
REGISTRY_HOST = os.getenv("REGISTRY_HOST")
REGISTRY_PORT = int(os.getenv("REGISTRY_PORT", 8080))
AWS_ACCESS_KEY = os.getenv("AWS_ACCESS_KEY_ID")
AWS_SECRET_KEY = os.getenv("AWS_SECRET_ACCESS_KEY")
S3_ENDPOINT = os.getenv("AWS_S3_ENDPOINT")

def log(msg): print(f"[PIPELINE]: {msg}")

def main():
    print(f"\n=== STEP 1: ACQUIRING ASSETS ===")
    log(f"Downloading '{MODEL_ID}' from Hugging Face...")
    # Using a cache dir explicitly to avoid permission issues in some UBI images
    local_dir = snapshot_download(repo_id=MODEL_ID, 
                                  cache_dir="/tmp/hf_cache",
                                  allow_patterns=["*.json", "*.safetensors", "*.model", "tokenizer*"])

    print(f"\n=== STEP 2: SECURING ASSETS (MINIO) ===")
    log(f"Connecting to Vault at {S3_ENDPOINT}...")
    
    s3 = boto3.client('s3',
                      endpoint_url=S3_ENDPOINT,
                      aws_access_key_id=AWS_ACCESS_KEY,
                      aws_secret_access_key=AWS_SECRET_KEY,
                      config=Config(signature_version='s3v4'))
    
    # Ensure bucket exists
    try:
        s3.create_bucket(Bucket=S3_BUCKET)
    except:
        pass 

    # Upload files
    s3_prefix = f"{MODEL_ID.replace('/', '-')}/{VERSION}"
    log(f"Uploading to s3://{S3_BUCKET}/{s3_prefix}...")
    
    for root, dirs, files in os.walk(local_dir):
        for file in files:
            local_path = os.path.join(root, file)
            relative_path = os.path.relpath(local_path, local_dir)
            s3_key = os.path.join(s3_prefix, relative_path)
            s3.upload_file(local_path, S3_BUCKET, s3_key)
            
    s3_uri = f"s3://{S3_BUCKET}/{s3_prefix}"
    log(f"Upload Complete: {s3_uri}")

    print(f"\n=== STEP 3: GOVERNANCE (MODEL REGISTRY) ===")
    log(f"Connecting to Registry at {REGISTRY_HOST}:{REGISTRY_PORT}...")

    # Connect to Registry using the official Python client
    registry = ModelRegistry(server_address=REGISTRY_HOST, port=REGISTRY_PORT, author="LabUser", is_secure=False)

    log(f"Registering Model: {MODEL_ID}")
    
    model = registry.register_model(
        MODEL_ID,
        s3_uri,
        model_format_name="safetensors",
        model_format_version="1.0",
        version=VERSION,
        description="Qwen3 Small Language Model imported from Hugging Face",
        metadata={
            "source": "huggingface",
            "original_repo": MODEL_ID,
            "license": "Apache 2.0",
            "is_governed": "true"
        }
    )

    print(f"\nâœ… SUCCESS: Supply Chain Complete!")
    print(f"    Model ID: {model.id}")
    print(f"    Version: {VERSION}")

if __name__ == "__main__":
    main()
EOF

echo "âž¤ Creating ConfigMap 'ingestion-code'..."
oc create configmap ingestion-code --from-file=ingest_and_register.py -n "$NAMESPACE" --dry-run=client -o yaml | oc apply -f -
rm ingest_and_register.py # Cleanup

# -----------------------------------------------------------------------------
# 3. Submit the Job
# -----------------------------------------------------------------------------
echo "âž¤ Submitting Kubernetes Job..."
cat <<YAML | oc apply -f -
apiVersion: batch/v1
kind: Job
metadata:
  name: model-ingest-job
  namespace: $NAMESPACE
spec:
  backoffLimit: 1
  template:
    spec:
      serviceAccountName: $SERVICE_ACCOUNT
      containers:
      - name: ingestor
        # Use Red Hat UBI Python image for better OpenShift compatibility
        image: registry.access.redhat.com/ubi9/python-39:latest
        command: ["/bin/bash", "-c"]
        args:
          - |
            echo "Installing dependencies..."
            # Install to user directory to avoid permission errors
            pip install boto3 huggingface-hub model-registry==0.2.3 requests --quiet --no-cache-dir
            echo "Starting Ingestion..."
            python /scripts/ingest_and_register.py
        volumeMounts:
        - name: code-volume
          mountPath: /scripts
        env:
        - name: REGISTRY_HOST
          value: "$REGISTRY_HOST"
        - name: REGISTRY_PORT
          value: "8080"
        # Inject credentials from the 'aws-connection-minio' secret
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: aws-connection-minio
              key: AWS_ACCESS_KEY_ID
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: aws-connection-minio
              key: AWS_SECRET_ACCESS_KEY
        - name: AWS_S3_ENDPOINT
          valueFrom:
            secretKeyRef:
              name: aws-connection-minio
              key: AWS_S3_ENDPOINT
      restartPolicy: Never
      volumes:
      - name: code-volume
        configMap:
          name: ingestion-code
YAML

echo "â³ Job submitted. Streaming logs..."
sleep 5
oc logs job/model-ingest-job -n "$NAMESPACE" -f
----

=== 2. Execute the Pipeline

Run the script to build and execute the job.

[source,bash]
----
chmod +x run_ingestion.sh
./run_ingestion.sh
----

=== 3. Verification

Once the job completes with `âœ… SUCCESS`, verify the results.

. **Check the S3 Vault:**
+
[source,bash]
----
# List objects in the private bucket
oc rsh -n rhoai-model-registry-lab deployment/minio -- mc ls local/private-models
----

. **Check the Model Registry:**
+
Navigate to the OpenShift AI Dashboard -> **Model Registry**. You should see the **Qwen/Qwen3-0.6B** model listed with Version 1.0.0.